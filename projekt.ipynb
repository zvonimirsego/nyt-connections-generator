{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9c400c8",
   "metadata": {},
   "source": [
    "# Projekt: Izrada jedne instance igrice *NYT Connections* pomoću NLP-a (biblioteke `nltk` i `gensim`)\n",
    "\n",
    "## Uvod\n",
    "\n",
    "*NYT Connections* jedna je od niza igara koje *New York Times* nudi svojim čitateljima. Njihove su igrice najčešće lingvističke prirode, od kojih je najpoznatija *Wordle*. *Connections* je igra koja je meni osobno najzanimljiva. Igrač dobije 16 riječi koje mora sklopiti u 4 skupine po 4 riječi, pri čemu sve riječi u skupini dijele nešto zajedničko. To nešto zajedničko što mogu imati može biti značenje, korištenje u istim frazemima, ali može biti i nešto vizualno kao npr. *superheroji, suci, boksači i vampiri svi nose plašteve*. Svaka skupina je teža od prethodne, a po težini poredane su sljedeće skupine: žuta, zelena, plava, ljubičasta. Cilj ovog projetka je napraviti jednu instancu takve igre pomoću tehnika naučenih na kolegiju *Računalno jezikoslovlje*, ali i nekim vlastitim istraživanjem mogućnosti koje nude biblioteke `nltk` i `gensim`, osobito po pitanju tehnike treniranja modela `Word2Vec`.\n",
    "\n",
    "## Primjer igre *Connections*\n",
    "\n",
    "U slikama u nastavku dan je primjer jedne instance originalnog *Connections*, kao i njegova točna rješenja. Dana je instanca igre od datuma 9.7.2025.\n",
    "\n",
    "![Connections](connections.png)\n",
    "\n",
    "![Connectionsrjesenja](connections_rjesenja.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28582b8d",
   "metadata": {},
   "source": [
    "Skinimo prvo datoteke koje ćemo koristiti tijekom projekta te postavimo neke vrijednosti koje ćemo koristiti tijekom projekta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f52d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a67b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b30e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be05084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import gensim.downloader as api\n",
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2e45bd",
   "metadata": {},
   "source": [
    "Sada postavimo skup riječi iz kojeg ćemo birati centralne riječi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa22743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import brown\n",
    "\n",
    "sentences = brown.sents()[:10000]\n",
    "\n",
    "filtered_words = [\n",
    "    word.lower() for sent in sentences for word in sent if word.isalnum() and word.lower() not in stop_words\n",
    "]\n",
    "random.shuffle(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc2905",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity(\"alice\", \"wonderland\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8123f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec = model['rome'] - model['italy'] + model['france']\n",
    "# model.most_similar(vec, topn = 5)\n",
    "model.most_similar(positive=[\"rome\", \"france\"], negative=[\"italy\"], topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cff063e",
   "metadata": {},
   "source": [
    "## Glavni program\n",
    "\n",
    "### Pomoćne funkcije\n",
    "\n",
    "Definirajmo prvo pomoćne funkcije koje ćemo koristiti tijekom generiranja jedne instance igre *Connections*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8fbc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which calculates how many letters the two words share in the order. Used to remove typos.\n",
    "def LCS_similarity(x : str, y : str):\n",
    "    table = []\n",
    "    n = len(x)\n",
    "    m = len(y)\n",
    "    for i in range(n+1):\n",
    "        row = []\n",
    "        for j in range(m+1):\n",
    "            row.append(-1)\n",
    "        table.append(row)\n",
    "    for i in range(m+1):\n",
    "        table[0][i] = 0\n",
    "    for i in range(n+1):\n",
    "        table[i][0] = 0\n",
    "    for i in range(1, n+1, 1):\n",
    "        for j in range(1, m+1, 1):\n",
    "            if x[i-1] == y[j-1]:\n",
    "                table[i][j] = table[i-1][j-1] + 1\n",
    "            else:\n",
    "                table[i][j] = max(table[i-1][j], table[i][j-1])\n",
    "    return table[n][m] / max(n, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569a3fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LCS_similarity('saturday', 'sunday')\n",
    "# suday -> 5/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f2573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# an auxiallry function which generates a similarity matrix\n",
    "def make_sim_matrix(words):\n",
    "    return [[model.similarity(w1, w2) for w2 in words] for w1 in words]\n",
    "\n",
    "# the function which displayes the similarity matrix\n",
    "def display_similarity_matrix(words):\n",
    "    similarity_matrix = make_sim_matrix(words)\n",
    "    df = pd.DataFrame(similarity_matrix, index=words, columns=words)\n",
    "    print(df.round(2).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdce5c4",
   "metadata": {},
   "source": [
    "### Glavni program\n",
    "\n",
    "Prijeđimo sada na glavni program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e723ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if an error occurs here because index \"j\" is out of range, just run this window again.\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# an axualliary function which transforms universal tags to wordnet tags, based on the given nltk.org data\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag == 'ADJ':\n",
    "        return wordnet.ADJ\n",
    "    elif tag == 'VERB':\n",
    "        return wordnet.VERB\n",
    "    elif tag == 'NOUN':\n",
    "        return wordnet.NOUN\n",
    "    elif tag == 'ADV':\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "cluster = []\n",
    "clusters = []\n",
    "words = []\n",
    "solutions = {}\n",
    "i = 0\n",
    "\n",
    "# the main programm which generates one Connections instance\n",
    "while len(clusters) < 4:\n",
    "    cluster = []\n",
    "    j = 0\n",
    "    main_word = random.choice(filtered_words).lower()\n",
    "    if main_word in model:\n",
    "        most_sim = model.most_similar(positive=[main_word], topn = 500)\n",
    "        if i == 0 and 0.8 <= most_sim[0][1]:\n",
    "            while len(cluster) < 4:\n",
    "                similar = False\n",
    "                word = most_sim[j][0].lower()\n",
    "                pair = pos_tag([word], tagset='universal')\n",
    "                word, pos = pair[0][0], pair[0][1]\n",
    "                word = lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "                for w in cluster:\n",
    "                    if LCS_similarity(word, w) >= 0.75:\n",
    "                        similar = True\n",
    "                if similar:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                sim = most_sim[j][1]\n",
    "                if word.isalnum() and word not in words:\n",
    "                    cluster.append(word)\n",
    "                    words.append(word)\n",
    "                j += 1\n",
    "            clusters.append(cluster)\n",
    "            solutions[main_word] = cluster\n",
    "            i += 1\n",
    "        elif i == 1 and 0.6 <= most_sim[0][1]:\n",
    "            while len(cluster) < 4:\n",
    "                similar = False\n",
    "                word = most_sim[j][0].lower()\n",
    "                pair = pos_tag([word], tagset='universal')\n",
    "                word, pos = pair[0][0], pair[0][1]\n",
    "                word = lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "                for w in cluster:\n",
    "                    if LCS_similarity(word, w) >= 0.75:\n",
    "                        similar = True\n",
    "                if similar:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                sim = most_sim[j][1]\n",
    "                if (word.isalnum() and word not in words and 0.6 <= sim < 0.8 and len(cluster) == 0) or (word.isalnum() and word not in words):\n",
    "                    cluster.append(word)\n",
    "                    words.append(word)\n",
    "                j += 1\n",
    "            clusters.append(cluster)\n",
    "            solutions[main_word] = cluster\n",
    "            i += 1\n",
    "        elif i == 2 and 0.5 <= most_sim[0][1]:\n",
    "            while len(cluster) < 4:\n",
    "                similar = False\n",
    "                word = most_sim[j][0].lower()\n",
    "                pair = pos_tag([word], tagset='universal')\n",
    "                word, pos = pair[0][0], pair[0][1]\n",
    "                word = lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "                for w in cluster:\n",
    "                    if LCS_similarity(word, w) >= 0.75:\n",
    "                        similar = True\n",
    "                if similar:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                sim = most_sim[j][1]\n",
    "                if (word.isalnum() and word not in words and 0.5 <= sim < 0.6 and len(cluster) == 0) or (word.isalnum() and word not in words):\n",
    "                    cluster.append(word)\n",
    "                    words.append(word)\n",
    "                j += 1\n",
    "            clusters.append(cluster)\n",
    "            solutions[main_word] = cluster\n",
    "            i += 1\n",
    "        elif i == 3 and 0 <= most_sim[0][1]:\n",
    "            while len(cluster) < 4:\n",
    "                similar = False\n",
    "                word = most_sim[j][0].lower()\n",
    "                pair = pos_tag([word], tagset='universal')\n",
    "                word, pos = pair[0][0], pair[0][1]\n",
    "                word = lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "                for w in cluster:\n",
    "                    if LCS_similarity(word, w) >= 0.75:\n",
    "                        similar = True\n",
    "                if similar:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                sim = most_sim[j][1]\n",
    "                if word.isalnum() and word not in words and sim < 0.5:\n",
    "                    cluster.append(word)\n",
    "                    words.append(word)\n",
    "                j += 1\n",
    "            clusters.append(cluster)\n",
    "            solutions[main_word] = cluster\n",
    "            i += 1\n",
    "\n",
    "for clus in clusters:\n",
    "    clus.sort()\n",
    "\n",
    "random.shuffle(words)\n",
    "\n",
    "word_table = [[words[4*i+j] for j in range(4)] for i in range(4)]\n",
    "df = pd.DataFrame(word_table)\n",
    "df.columns = [''] * df.shape[1]\n",
    "df.index = [''] * df.shape[0]\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4818932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to play:\n",
    "# Write your guess in the appropriate area as 4 words, seperated by space. \n",
    "# Extra spaces will be removed.\n",
    "# If you type in 'hint', the similarity matrix will show up\n",
    "\n",
    "import time\n",
    "\n",
    "attempts_left = 4\n",
    "solved = False\n",
    "guessed = 0\n",
    "hints_used = 0\n",
    "\n",
    "wordss = []\n",
    "for word in words:\n",
    "    wordss.append(word)\n",
    "\n",
    "for key, value in solutions.items():\n",
    "    value.sort()\n",
    "\n",
    "while attempts_left > 0 and not solved:\n",
    "    wordss_table = [[wordss[4*i+j] for j in range(4)] for i in range(int(len(wordss)/4))]\n",
    "    wt = pd.DataFrame(wordss_table)\n",
    "    wt.columns = [''] * wt.shape[1]\n",
    "    wt.index = [''] * wt.shape[0]\n",
    "    print(wt.to_string())\n",
    "    print(f\"Attempts left: {attempts_left}\")\n",
    "    attempt = input().split(\" \")\n",
    "    attempt.sort()\n",
    "    if attempt == ['hint']:\n",
    "        display_similarity_matrix(wordss)\n",
    "        hints_used += 1\n",
    "        continue\n",
    "    for word in attempt:\n",
    "        if word == '':\n",
    "            attempt.remove(word)\n",
    "    time.sleep(1)\n",
    "    if attempt in clusters:\n",
    "        guessed += 1\n",
    "        print(\"Correct! You've guessed a cluster!\")\n",
    "        for key, value in solutions.items():\n",
    "            if value == attempt:\n",
    "                print(f\"{key} : {value}\")\n",
    "        for word in attempt:\n",
    "            wordss.remove(word)\n",
    "        time.sleep(1)\n",
    "        random.shuffle(wordss)\n",
    "    else:\n",
    "        print(\"Wrong! Try again!\")\n",
    "        attempts_left -= 1\n",
    "        time.sleep(1)\n",
    "    if len(wordss) == 0 or guessed == 4:\n",
    "        solved = True\n",
    "    print()\n",
    "print(\"GAME OVER!\")\n",
    "if solved:\n",
    "    print(\"Congrats! You're so good at this!\")\n",
    "else:\n",
    "    print(\"Better luck next time!\")\n",
    "print()\n",
    "for key, cluster in solutions.items():\n",
    "    for word in cluster:\n",
    "        print(word, end=\" \")\n",
    "    print(f\"- {key}\")\n",
    "print(f\"Hints used: {hints_used}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
