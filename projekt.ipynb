{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9c400c8",
   "metadata": {},
   "source": [
    "# Projekt: Izrada jedne instance igrice *NYT Connections* pomoću NLP-a (biblioteke `nltk` i `gensim`)\n",
    "\n",
    "## Uvod\n",
    "\n",
    "*NYT Connections* jedna je od niza igara koje *New York Times* nudi svojim čitateljima. Njihove su igrice najčešće lingvističke prirode, od kojih je najpoznatija *Wordle*. *Connections* je igra koja je meni osobno najzanimljiva. Igrač dobije 16 riječi koje mora sklopiti u 4 skupine po 4 riječi, pri čemu sve riječi u skupini dijele nešto zajedničko. To nešto zajedničko što mogu imati može biti značenje, korištenje u istim frazemima, ali može biti i nešto vizualno kao npr. *superheroji, suci, boksači i vampiri svi nose plašteve*. Svaka skupina je teža od prethodne, a po težini poredane su sljedeće skupine: žuta, zelena, plava, ljubičasta. Cilj ovog projetka je napraviti jednu instancu takve igre pomoću tehnika naučenih na kolegiju *Računalno jezikoslovlje*, ali i nekim vlastitim istraživanjem mogućnosti koje nude biblioteke `nltk` i `gensim`, osobito po pitanju tehnike treniranja modela `Word2Vec`.\n",
    "\n",
    "## Primjer igre *Connections*\n",
    "\n",
    "U slikama u nastavku dan je primjer jedne instance originalnog *Connections*, kao i njegova točna rješenja. Dana je instanca igre od datuma 9.7.2025.\n",
    "\n",
    "![Connections](connections.png)\n",
    "\n",
    "![Connectionsrjesenja](connections_rjesenja.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28582b8d",
   "metadata": {},
   "source": [
    "Skinimo prvo datoteke koje ćemo koristiti tijekom projekta te postavimo neke vrijednosti koje ćemo koristiti tijekom projekta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6f52d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\zvonimir šego\\desktop\\projekt\\projekt\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.2.1 joblib-1.5.1 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "001a67b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-4.3.3-cp311-cp311-win_amd64.whl.metadata (8.2 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Using cached scipy-1.13.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Using cached smart_open-7.3.0.post1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Using cached wrapt-1.17.2-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Using cached gensim-4.3.3-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Using cached scipy-1.13.1-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "Using cached smart_open-7.3.0.post1-py3-none-any.whl (61 kB)\n",
      "Using cached wrapt-1.17.2-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
      "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.3.0.post1 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48b30e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.3.1-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\zvonimir šego\\desktop\\projekt\\projekt\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zvonimir šego\\desktop\\projekt\\projekt\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zvonimir šego\\desktop\\projekt\\projekt\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.3.1-cp311-cp311-win_amd64.whl (11.3 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.3.1 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be05084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Zvonimir\n",
      "[nltk_data]     Šego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import gensim.downloader as api\n",
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2e45bd",
   "metadata": {},
   "source": [
    "Sada postavimo skup riječi iz kojeg ćemo birati centralne riječi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa22743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import brown\n",
    "\n",
    "sentences = brown.sents()[:10000]\n",
    "\n",
    "filtered_words = [\n",
    "    word.lower() for sent in sentences for word in sent if word.isalnum() and word.lower() not in stop_words\n",
    "]\n",
    "random.shuffle(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11dc2905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3020006"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"alice\", \"wonderland\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e8123f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paris', 0.47690874338150024),\n",
       " ('charles', 0.474092036485672),\n",
       " ('albert', 0.46750128269195557),\n",
       " ('alors', 0.4648790955543518),\n",
       " ('georgia', 0.4646168351173401)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vec = model['rome'] - model['italy'] + model['france']\n",
    "# model.most_similar(vec, topn = 5)\n",
    "model.most_similar(positive=[\"rome\", \"france\"], negative=[\"italy\"], topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cff063e",
   "metadata": {},
   "source": [
    "## Glavni program\n",
    "\n",
    "### Pomoćne funkcije\n",
    "\n",
    "Definirajmo prvo pomoćne funkcije koje ćemo koristiti tijekom generiranja jedne instance igre *Connections*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a8fbc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which calculates how many letters the two words share in the order. Used to remove typos.\n",
    "def LCS_similarity(x : str, y : str):\n",
    "    table = []\n",
    "    n = len(x)\n",
    "    m = len(y)\n",
    "    for i in range(n+1):\n",
    "        row = []\n",
    "        for j in range(m+1):\n",
    "            row.append(-1)\n",
    "        table.append(row)\n",
    "    for i in range(m+1):\n",
    "        table[0][i] = 0\n",
    "    for i in range(n+1):\n",
    "        table[i][0] = 0\n",
    "    for i in range(1, n+1, 1):\n",
    "        for j in range(1, m+1, 1):\n",
    "            if x[i-1] == y[j-1]:\n",
    "                table[i][j] = table[i-1][j-1] + 1\n",
    "            else:\n",
    "                table[i][j] = max(table[i-1][j], table[i][j-1])\n",
    "    return table[n][m] / max(n, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "569a3fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LCS_similarity('saturday', 'sunday')\n",
    "# suday -> 5/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76f2573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# an auxiallry function which generates a similarity matrix\n",
    "def make_sim_matrix(words):\n",
    "    return [[model.similarity(w1, w2) for w2 in words] for w1 in words]\n",
    "\n",
    "# the function which displayes the similarity matrix\n",
    "def display_similarity_matrix(words):\n",
    "    similarity_matrix = make_sim_matrix(words)\n",
    "    df = pd.DataFrame(similarity_matrix, index=words, columns=words)\n",
    "    print(df.round(2).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdce5c4",
   "metadata": {},
   "source": [
    "### Glavni program\n",
    "\n",
    "Prijeđimo sada na glavni program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e723ae50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "  communion           me  sacrament     ourselves\n",
      "         ag  agriculture       true      actually\n",
      "         we         kind   definite  agribusiness\n",
      "  eucharist         them       farm       liturgy\n"
     ]
    }
   ],
   "source": [
    "# if an error occurs here because index \"j\" is out of range, just run this window again.\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# an axualliary function which transforms universal tags to wordnet tags, based on the given nltk.org data\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag == 'ADJ':\n",
    "        return wordnet.ADJ\n",
    "    elif tag == 'VERB':\n",
    "        return wordnet.VERB\n",
    "    elif tag == 'NOUN':\n",
    "        return wordnet.NOUN\n",
    "    elif tag == 'ADV':\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "cluster = []\n",
    "clusters = []\n",
    "words = []\n",
    "solutions = {}\n",
    "i = 0\n",
    "\n",
    "# the main programm which generates one Connections instance\n",
    "while len(clusters) < 4:\n",
    "    cluster = []\n",
    "    j = 0\n",
    "    main_word = random.choice(filtered_words).lower()\n",
    "    if main_word in model:\n",
    "        most_sim = model.most_similar(positive=[main_word], topn = 500)\n",
    "        if i == 0 and 0.8 <= most_sim[0][1]:\n",
    "            while len(cluster) < 4:\n",
    "                similar = False\n",
    "                word = most_sim[j][0].lower()\n",
    "                pair = pos_tag([word], tagset='universal')\n",
    "                word, pos = pair[0][0], pair[0][1]\n",
    "                word = lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "                for w in cluster:\n",
    "                    if LCS_similarity(word, w) >= 0.75:\n",
    "                        similar = True\n",
    "                if similar:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                sim = most_sim[j][1]\n",
    "                if word.isalnum() and word not in words:\n",
    "                    cluster.append(word)\n",
    "                    words.append(word)\n",
    "                j += 1\n",
    "            clusters.append(cluster)\n",
    "            solutions[main_word] = cluster\n",
    "            i += 1\n",
    "        elif i == 1 and 0.6 <= most_sim[0][1]:\n",
    "            while len(cluster) < 4:\n",
    "                similar = False\n",
    "                word = most_sim[j][0].lower()\n",
    "                pair = pos_tag([word], tagset='universal')\n",
    "                word, pos = pair[0][0], pair[0][1]\n",
    "                word = lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "                for w in cluster:\n",
    "                    if LCS_similarity(word, w) >= 0.75:\n",
    "                        similar = True\n",
    "                if similar:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                sim = most_sim[j][1]\n",
    "                if (word.isalnum() and word not in words and 0.6 <= sim < 0.8 and len(cluster) == 0) or (word.isalnum() and word not in words):\n",
    "                    cluster.append(word)\n",
    "                    words.append(word)\n",
    "                j += 1\n",
    "            clusters.append(cluster)\n",
    "            solutions[main_word] = cluster\n",
    "            i += 1\n",
    "        elif i == 2 and 0.5 <= most_sim[0][1]:\n",
    "            while len(cluster) < 4:\n",
    "                similar = False\n",
    "                word = most_sim[j][0].lower()\n",
    "                pair = pos_tag([word], tagset='universal')\n",
    "                word, pos = pair[0][0], pair[0][1]\n",
    "                word = lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "                for w in cluster:\n",
    "                    if LCS_similarity(word, w) >= 0.75:\n",
    "                        similar = True\n",
    "                if similar:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                sim = most_sim[j][1]\n",
    "                if (word.isalnum() and word not in words and 0.5 <= sim < 0.6 and len(cluster) == 0) or (word.isalnum() and word not in words):\n",
    "                    cluster.append(word)\n",
    "                    words.append(word)\n",
    "                j += 1\n",
    "            clusters.append(cluster)\n",
    "            solutions[main_word] = cluster\n",
    "            i += 1\n",
    "        elif i == 3 and 0 <= most_sim[0][1]:\n",
    "            while len(cluster) < 4:\n",
    "                similar = False\n",
    "                word = most_sim[j][0].lower()\n",
    "                pair = pos_tag([word], tagset='universal')\n",
    "                word, pos = pair[0][0], pair[0][1]\n",
    "                word = lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "                for w in cluster:\n",
    "                    if LCS_similarity(word, w) >= 0.75:\n",
    "                        similar = True\n",
    "                if similar:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                sim = most_sim[j][1]\n",
    "                if word.isalnum() and word not in words and sim < 0.5:\n",
    "                    cluster.append(word)\n",
    "                    words.append(word)\n",
    "                j += 1\n",
    "            clusters.append(cluster)\n",
    "            solutions[main_word] = cluster\n",
    "            i += 1\n",
    "\n",
    "for clus in clusters:\n",
    "    clus.sort()\n",
    "\n",
    "random.shuffle(words)\n",
    "\n",
    "word_table = [[words[4*i+j] for j in range(4)] for i in range(4)]\n",
    "df = pd.DataFrame(word_table)\n",
    "df.columns = [''] * df.shape[1]\n",
    "df.index = [''] * df.shape[0]\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4818932c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "  communion           me  sacrament     ourselves\n",
      "         ag  agriculture       true      actually\n",
      "         we         kind   definite  agribusiness\n",
      "  eucharist         them       farm       liturgy\n",
      "Attempts left: 4\n",
      "Correct! You've guessed a cluster!\n",
      "communion : ['communion', 'eucharist', 'liturgy', 'sacrament']\n",
      "\n",
      "                                       \n",
      "  agribusiness  true     actually  farm\n",
      "          kind    ag    ourselves  them\n",
      "      definite    me  agriculture    we\n",
      "Attempts left: 4\n",
      "Correct! You've guessed a cluster!\n",
      "agricultural : ['ag', 'agribusiness', 'agriculture', 'farm']\n",
      "\n",
      "                                 \n",
      "        we  kind   actually    me\n",
      "  definite  them  ourselves  true\n",
      "Attempts left: 4\n",
      "             we  kind  actually    me  definite  them  ourselves  true\n",
      "we         1.00  0.45      0.45  0.45      0.26  0.46       0.66  0.28\n",
      "kind       0.45  1.00      0.50  0.47      0.33  0.31       0.35  0.34\n",
      "actually   0.45  0.50      1.00  0.44      0.26  0.42       0.27  0.42\n",
      "me         0.45  0.47      0.44  1.00      0.19  0.60       0.44  0.27\n",
      "definite   0.26  0.33      0.26  0.19      1.00  0.11       0.13  0.34\n",
      "them       0.46  0.31      0.42  0.60      0.11  1.00       0.56  0.14\n",
      "ourselves  0.66  0.35      0.27  0.44      0.13  0.56       1.00  0.17\n",
      "true       0.28  0.34      0.42  0.27      0.34  0.14       0.17  1.00\n",
      "                                 \n",
      "        we  kind   actually    me\n",
      "  definite  them  ourselves  true\n",
      "Attempts left: 4\n",
      "Correct! You've guessed a cluster!\n",
      "us : ['me', 'ourselves', 'them', 'we']\n",
      "\n",
      "                                \n",
      "  definite  true  kind  actually\n",
      "Attempts left: 4\n",
      "Correct! You've guessed a cluster!\n",
      "real : ['actually', 'definite', 'kind', 'true']\n",
      "\n",
      "GAME OVER!\n",
      "Congrats! You're so good at this!\n",
      "\n",
      "ag agribusiness agriculture farm - agricultural\n",
      "communion eucharist liturgy sacrament - communion\n",
      "me ourselves them we - us\n",
      "actually definite kind true - real\n",
      "Hints used: 1\n"
     ]
    }
   ],
   "source": [
    "# How to play:\n",
    "# Write your guess in the appropriate area as 4 words, seperated by space. \n",
    "# Extra spaces will be removed.\n",
    "# If you type in 'hint', the similarity matrix will show up\n",
    "\n",
    "import time\n",
    "\n",
    "attempts_left = 4\n",
    "solved = False\n",
    "guessed = 0\n",
    "hints_used = 0\n",
    "\n",
    "wordss = []\n",
    "for word in words:\n",
    "    wordss.append(word)\n",
    "\n",
    "for key, value in solutions.items():\n",
    "    value.sort()\n",
    "\n",
    "while attempts_left > 0 and not solved:\n",
    "    wordss_table = [[wordss[4*i+j] for j in range(4)] for i in range(int(len(wordss)/4))]\n",
    "    wt = pd.DataFrame(wordss_table)\n",
    "    wt.columns = [''] * wt.shape[1]\n",
    "    wt.index = [''] * wt.shape[0]\n",
    "    print(wt.to_string())\n",
    "    print(f\"Attempts left: {attempts_left}\")\n",
    "    attempt = input().split(\" \")\n",
    "    attempt.sort()\n",
    "    if attempt == ['hint']:\n",
    "        display_similarity_matrix(wordss)\n",
    "        hints_used += 1\n",
    "        continue\n",
    "    for word in attempt:\n",
    "        if word == '':\n",
    "            attempt.remove(word)\n",
    "    time.sleep(1)\n",
    "    if attempt in clusters:\n",
    "        guessed += 1\n",
    "        print(\"Correct! You've guessed a cluster!\")\n",
    "        for key, value in solutions.items():\n",
    "            if value == attempt:\n",
    "                print(f\"{key} : {value}\")\n",
    "        for word in attempt:\n",
    "            wordss.remove(word)\n",
    "        time.sleep(1)\n",
    "        random.shuffle(wordss)\n",
    "    else:\n",
    "        print(\"Wrong! Try again!\")\n",
    "        attempts_left -= 1\n",
    "        time.sleep(1)\n",
    "    if len(wordss) == 0 or guessed == 4:\n",
    "        solved = True\n",
    "    print()\n",
    "print(\"GAME OVER!\")\n",
    "if solved:\n",
    "    print(\"Congrats! You're so good at this!\")\n",
    "else:\n",
    "    print(\"Better luck next time!\")\n",
    "print()\n",
    "for key, cluster in solutions.items():\n",
    "    for word in cluster:\n",
    "        print(word, end=\" \")\n",
    "    print(f\"- {key}\")\n",
    "print(f\"Hints used: {hints_used}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
